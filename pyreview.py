from transformers import AutoModelForCausalLM, AutoTokenizer
import sys
import argparse
import os
import functools


PROMPT = """
Can you give me some feedback on my Python code?

```python
{code}
```

- Please give me AT MOST 3 points of improvement.
- No fluff and clich√©s; refer to specific parts of my code! 
- DON'T write example code for me.
- Don't recommend any package outside the Python standard library.
{nudge}
Thanks so much!
""".strip()

MAX_NEW_TOKENS = 1024

PREFIX = """
This feedback was generated by a large language model (LLM): {model}. 

- LLMs are often wrong; they may not understand the intent of your code.
- The model was not given the assignment, only your code.
- The model was not given information about your current knowledge of Python or the course contents.

##################################################
"""

def main():

    argparser = argparse.ArgumentParser(description='Auto-review Python code for beginners.')
    argparser.add_argument('files', nargs='*', default=[sys.stdin], type=argparse.FileType('r'))
    argparser.add_argument('--model', nargs='?', default="Qwen/CodeQwen1.5-7B-Chat", type=str)
    argparser.add_argument('--force', required=False, action='store_true')
    argparser.add_argument('--nudge', nargs='*', type=str)
    argparser.add_argument('--prefix', required=False, type=str, default=PREFIX)

    args = argparser.parse_args()
    if args.prefix and '{model}' in args.prefix:
        args.prefix = args.prefix.format(model=args.model)
    if args.nudge:
        args.nudge = ''.join(f'- {nudge}\n' for nudge in args.nudge)

    programs = [file.read() for file in args.files]

    prompt_format = functools.partial(PROMPT.format, nudge=args.nudge)

    device = "cuda"  # the device to load the model onto
    model = AutoModelForCausalLM.from_pretrained(
        args.model,
        torch_dtype="auto",
        device_map="auto"
    )
    # model = model.to(device)    # not needed?
    tokenizer = AutoTokenizer.from_pretrained(args.model)
    model_inputs = build_model_inputs(programs, tokenizer, prompt_format).to(device)
    model_outputs = model.generate(model_inputs.input_ids, max_new_tokens=MAX_NEW_TOKENS)
    generated_ids = (output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, model_outputs))

    responses = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)

    if args.files == '-':
        if args.prefix:
            print(args.prefix)
        print(responses[0])
    else:
        for path, response in zip(args.files, responses):
            outpath = path.replace('.py', '.md')
            if os.path.exists(outpath) and not args.force:
                raise FileExistsError(f'Feedback file exists: {outpath}! Use --force to overwrite.')
            with open(outpath, 'w') as outfile:
                if args.prefix:
                    outfile.write(args.prefix)
                outfile.write(response)


def build_model_inputs(programs, tokenizer, prompt_format):
    texts = []

    for program in programs:
        messages = [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt_format(code=program).strip()}
        ]
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        texts.append(text)

    return tokenizer(texts, return_tensors="pt")


if __name__ == '__main__':
    main()